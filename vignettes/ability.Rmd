---
title: "Replication Codes for `Does AI help humans make better decisions?`"
author: "Sooahn Shin"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ability}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.height = 6, fig.width = 9,
  message = FALSE
)
options(rmarkdown.html_vignette.check_title = FALSE)
```

```{r setup}
library(aihuman)
library(tidyr)
library(dplyr)
library(ggplot2)
## set default ggplot theme
theme_set(theme_bw(base_size = 15) + theme(plot.title = element_text(hjust = 0.5)))
```

Ben-Michael, Eli, D. James Greiner, Melody Huang, Kosuke Imai, Zhichao Jiang, and Sooahn Shin. ["Does AI help humans make better decisions?: A statistical evaluation framework for experimental and observational studies"](https://arxiv.org/pdf/2403.12108), 2024.

The data used for this paper, and made available here, are interim, based on only half of the observations in the study and (for those observations) only half of the study follow-up period.  We use them only to illustrate methods, not to draw substantive conclusions.

## Overview

In this vignette, we will use the data originally from [Imai, Jiang, Greiner, Halen, and Shin (2023)](https://doi.org/10.1093/jrsssa/qnad010).
The essential part of the data is:

* `Z`: A binary treatment $Z_i \in \{0,1\}$ (e.g. PSA provision)
* `D`: A binary decision $D_i \in \{0,1\}$ (e.g. judge's release decision)
* `A`: A binary AI recommendation $A_i \in \{0,1\}$ (e.g. dichotomized pretrial public safety assessment scores)
* `Y`: A binary outcome $Y_i \in \{0,1\}$ (e.g. new criminal activity)
* Pre-treatment covariates $X_i$ (optional)

The analysis can be conducted based on the following workflow:

1. Data preparation and descriptive analysis
2. Human+AI v. Human comparison
3. AI v. Human comparison
4. Different loss functions
5. Policy Learning

## Data Preparation \& Descriptive Analysis

We will be using the following data:

```{r data_main}
# randomized PSA provision (0: none, 1: provided)
Z <- aihuman::NCAdata$Z
# judge's release decision (0: signature, 1: cash)
D <- if_else(aihuman::NCAdata$D == 0, 0, 1)
# dichotomized pretrial public safety assessment scores (0: signature, 1: cash)
A <- aihuman::PSAdata$DMF
# new criminal activity (0: no, 1: yes)
Y <- aihuman::NCAdata$Y
```

For subgroup analysis, we will use the following covariates:

```{r data_subgroup}
# race
race_vec <- aihuman::NCAdata |> 
  mutate(race = if_else(White == 1, "White", "Non-white")) |>
  pull(race)
# gender
gender_vec <- aihuman::NCAdata |> 
  mutate(gender = if_else(Sex == 1, "Male", "Female")) |>
  pull(gender)
```

### Nuisance functions

In the paper, we use doubly robust estimators throught the analysis. 
For this purpose, we will be fitting the nuisance functions using the following covariates ($X_i$):

```{r cov}
cov_mat <- aihuman::NCAdata |> 
  select(-c(Y, D, Z)) |> 
  bind_cols(FTAScore = aihuman::PSAdata$FTAScore, 
            NCAScore = aihuman::PSAdata$NCAScore, 
            NVCAFlag = aihuman::PSAdata$NVCAFlag) |>
  as.matrix()
colnames(cov_mat)
```

Then, the first set of the nuisance functions can be computed as follows, based on the Generalized Boosted Regression Modeling (see `gbm::gbm` function):

```{r nuisance, eval = FALSE}
nuis_func <- compute_nuisance_functions(Y = Y, D = D, Z = Z, V = cov_mat, shrinkage = 0.01, n.trees = 1000)
```

This gives us the following nuisance functions:

- The decision model $m^{D}(z, x) := \Pr(D = 1 \mid Z = z, X = x)$, for each treatment group $z \in \{0,1\}$. `nuis_func$z_models$d_pred` gives estimated $\hat{m}^{D}(z, X_i)$.
- The outcome model $m^{Y}(z, x) := \Pr(Y = 1 \mid D = 0, Z = z, X = x)$, for each treatment group $z \in \{0,1\}$. `nuis_func$z_models$y_pred` gives estimated $\hat{m}^{Y}(z, X_i)$.
- The propensity score model $e(z, X_i) := \Pr(Z = z \mid X = X_i)$. `nuis_func$pscore` gives estimated $\hat{e}(1, X_i)$.

The second set of the nuisance functions *conditioning on AI recommendation* can be computed similarly:

```{r nuisance_ai, eval = FALSE}
nuis_func_ai <- compute_nuisance_functions_ai(Y = Y, D = D, Z = Z, A = A, V = cov_mat, shrinkage = 0.01, n.trees = 1000)
```

This gives us the following nuisance functions:

- The decision model $m^{D}(z, a, x) := \Pr(D = 1 \mid Z = z, A = a, X = x)$, for each treatment group $z \in \{0,1\}$ and AI recommendation $a \in \{0,1\}$. `nuis_func_ai$z_models$d_pred` gives estimated $\hat{m}^{D}(z, a, X_i)$.
- The outcome model $m^{Y}(z, a, x) := \Pr(Y = 1 \mid D = 0, Z = z, A = a, X = x)$, for each treatment group $z \in \{0,1\}$ and AI recommendation $a \in \{0,1\}$. `nuis_func_ai$z_models$y_pred` gives estimated $\hat{m}^{Y}(z, a, X_i)$.
- The propensity score model $e(z, X_i) := \Pr(Z = z \mid X = X_i)$. `nuis_func_ai$pscore` gives estimated $\hat{e}(1, X_i)$.

For the reproducibility, we will be using the pre-computed nuisance functions, generated from the same codes above.

### Contingency table of human decisions and PSA recommendations

Comparison between human decisions and PSA-generated recommendations are summarized in the following contingency table (Table 1 in the paper).

```{r table_human}
counts <- table(D[Z == 0], A[Z == 0])
proportions <- prop.table(counts) * 100  
combined_table_human <- matrix(sprintf("%.1f%% (%d)", proportions, counts),
                         nrow = nrow(counts), ncol = ncol(counts))
colnames(combined_table_human) <- c("Signature", "Cash")
rownames(combined_table_human) <- c("Signature", "Cash")
knitr::kable(combined_table_human, caption = "Table 1 (Human v. PSA)")
```

```{r table_human_ai}
counts <- table(D[Z == 1], A[Z == 1])
proportions <- prop.table(counts) * 100  
combined_table_human_ai <- matrix(sprintf("%.1f%% (%d)", proportions, counts),
                         nrow = nrow(counts), ncol = ncol(counts))
colnames(combined_table_human_ai) <- c("Signature", "Cash")
rownames(combined_table_human_ai) <- c("Signature", "Cash")
knitr::kable(combined_table_human_ai, caption = "Table 1 (Human+PSA v. PSA)")
```

### Agreement between human decisions and PSA recommendations

We can analyze the impact of AI recommendations on agreement between human decisions and AI recommendations using the difference in means estimates of an indicator $1\{D_i = A_i\}$. The results are as follows (Figure S1 in the appendix).

```{r agreement}
table_agreement(
  Y = Y,
  D = D,
  Z = Z,
  A = A,
  subgroup1 = race_vec,
  subgroup2 = gender_vec,
  label.subgroup1 = "Race",
  label.subgroup2 = "Gender"
) |>
  mutate_at(vars(agree_diff, agree_diff_se, ci_ub, ci_lb), ~round(., 3)) |>
  knitr::kable(caption = "Agreement of Human and PSA Decision Makers")
plot_agreement(
  Y = Y,
  D = D,
  Z = Z,
  A = A,
  subgroup1 = race_vec,
  subgroup2 = gender_vec,
  label.subgroup1 = "Race",
  label.subgroup2 = "Gender",
  x.order = c("Overall", "Non-white", "White", "Female", "Male"),
  p.title = "Agreement between Human Decisions and PSA Recommendations", p.lb = -0.25, p.ub = 0.25
)
```

## Human+AI v. Human comparison

We now compare difference in risk between human+AI and human decision makers using AIPW estimators. 
You may choose between (1) AIPW estimator and (2) difference-in-means (Neyman) estimator.
You can also specify the loss ratio between false positives and false negatives using `l01` parameter.
For the subgroup analysis, you can specify the subgroup variable using `X`.

```{r diff_human}
# AIPW estimator
compute_stats_aipw(
  Y = Y,
  D = D,
  Z = Z,
  nuis_funcs = nuis_func, 
  X = NULL,
  l01 = 1
)
# Difference-in-means (Neyman) estimator
compute_stats(
  Y = Y,
  D = D,
  Z = Z,
  X = NULL,
  l01 = 1
)
```

The results can be visualized as follows (Figure 1 in the paper). You may use `plot_diff_human()` function for the Neyman estimator.

```{r diff_human_vis}
plot_diff_human_aipw(
  Y = Y,
  D = D,
  Z = Z,
  nuis_funcs = nuis_func, 
  l01 = 1,
  true.pscore = rep(0.5, length(D)),
  subgroup1 = race_vec,
  subgroup2 = gender_vec,
  label.subgroup1 = "Race",
  label.subgroup2 = "Gender",
  x.order = c("Overall", "Non-white", "White", "Female", "Male"),
  p.title = NULL, p.lb = -0.3, p.ub = 0.3
)
```

### How the human overrides the AI recommendation?

We can answer the question, "How the human overrides the AI recommendation?" using the following subgroup analysis defined by `A`.

First, subgroup analysis for the cases where AI recommends harsher decision ($A = 1$). The figure shows how the human overrides the AI recommendation in terms of true negative proportion (TNP), false negative proportion (FNP), and their differences (Figure S2 in the appendix).

```{r diff_human_vis_override1}
plot_diff_subgroup(
  Y = Y,
  D = D,
  Z = Z,
  A = A,
  a = 1,
  l01 = 1,
  nuis_funcs = nuis_func,
  true.pscore = rep(0.5, length(D)),
  subgroup1 = race_vec,
  subgroup2 = gender_vec,
  label.subgroup1 = "Race",
  label.subgroup2 = "Gender",
  x.order = c("Overall", "Non-white", "White", "Female", "Male"),
  p.title = NULL, p.lb = -0.5, p.ub = 0.5,
  label = "TNP - FNP",
  metrics = c("True Negative Proportion (TNP)", "False Negative Proportion (FNP)", "TNP - FNP")
)
```

Second, subgroup analysis for the cases where AI recommends lenient decision ($A = 0$) is also available.
The figure shows how human overrides the AI recommendation in terms of true positive proportion (TPP), false positive proportion (FPP), and their differences (Figure S3 in the appendix).

```{r diff_human_vis_override0}
plot_diff_subgroup(
  Y = Y,
  D = D,
  Z = Z,
  A = A,
  a = 0,
  l01 = 1,
  nuis_funcs = nuis_func,
  true.pscore = rep(0.5, length(D)),
  subgroup1 = race_vec,
  subgroup2 = gender_vec,
  label.subgroup1 = "Race",
  label.subgroup2 = "Gender",
  x.order = c("Overall", "Non-white", "White", "Female", "Male"),
  p.title = NULL, p.lb = -0.5, p.ub = 0.5,
  label = "TPP - FPP",
  metrics = c("True Positive Proportion (TPP)", "False Positive Proportion (FPP)", "TPP - FPP")
)
```


